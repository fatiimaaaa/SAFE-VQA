{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "id": "4sdblGRd8s76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV4ba1sk3skd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check current working directory\n",
        "os.chdir('/content/drive/My Drive/Handxray_Dataset/handxray')\n",
        "print(\"Current Working Directory:\", os.getcwd())\n"
      ],
      "metadata": {
        "id": "TCvIYAdN4IZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import ast"
      ],
      "metadata": {
        "id": "-szAWA0s5jz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "1Ys3w2I85m2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DataFrame\n",
        "file_path = 'VQA_QAEncoded.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "IvCoSW-B5nyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure question_encoded and answer_encoded are properly formatted\n",
        "df['question_encoded'] = df['question_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "df['answer_encoded'] = df['answer_encoded'].astype(int)\n"
      ],
      "metadata": {
        "id": "-yQVK8RF6IiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure all answers in your dataset are integers and lie in the range [0, answer_vocab_size - 1]\n",
        "print(\"Unique answer labels:\", df['answer_encoded'].unique())\n",
        "print(\"Answer vocabulary size:\", len(df['answer_encoded'].unique()))\n"
      ],
      "metadata": {
        "id": "Q5aa-S5v525F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build question_vocab from unique tokens in questions\n",
        "question_vocab = set(token for q in df['question_encoded'] for token in q)\n",
        "question_vocab = {token: idx for idx, token in enumerate(question_vocab)}"
      ],
      "metadata": {
        "id": "vKwvotW46Cus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tnuEkRRDP_Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check the final layer's output dimension:\n",
        "print(\"Answer vocab size used in model:\", len(df['answer_encoded'].unique()))\n"
      ],
      "metadata": {
        "id": "9RdJsaS66ziR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom dataset\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Image\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Load Question and Answer\n",
        "        question = torch.tensor(self.dataframe.iloc[idx]['question_encoded'], dtype=torch.long)\n",
        "        answer = torch.tensor(self.dataframe.iloc[idx]['answer_encoded'], dtype=torch.long)\n",
        "\n",
        "\n",
        "        return image, question, answer"
      ],
      "metadata": {
        "id": "uMygUW6QDNf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Answer range:\", df['answer_encoded'].min(), df['answer_encoded'].max())\n",
        "print(\"Question vocab size:\", len(question_vocab))\n"
      ],
      "metadata": {
        "id": "cGP0OLdUGx-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(question_vocab)"
      ],
      "metadata": {
        "id": "NauxLKljW03i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "kGqqwPejIiW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset and dataloader\n",
        "dataset = VQADataset(df, transform=transform)\n",
        "\n",
        "# Collate function for padding questions\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for item in batch:\n",
        "        images.append(item[0])\n",
        "        questions.append(item[1])\n",
        "        answers.append(item[2])\n",
        "\n",
        "        # Stack images and pad questions\n",
        "    images = torch.stack(images)\n",
        "    questions = pad_sequence(questions, batch_first=True, padding_value=0)\n",
        "    answers = torch.tensor(answers, dtype=torch.long)\n",
        "\n",
        "    return images, questions, answers\n",
        "\n",
        "#dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "68lGGHKuIoMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "vsL03oDR3O44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8eeDSl3DA8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, question_vocab_size, hidden_size, answer_vocab_size, resnet_type='resnet152'):\n",
        "        super(VQAModel, self).__init__()\n",
        "\n",
        "        # Load the specified ResNet model\n",
        "        self.resnet = self._get_resnet_model(resnet_type, hidden_size)\n",
        "\n",
        "        # Embedding for questions\n",
        "        self.embedding = nn.Embedding(question_vocab_size, hidden_size)\n",
        "\n",
        "        # LSTM for question processing\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "        # Final fully connected layer to predict the answer\n",
        "        self.fc = nn.Linear(hidden_size * 2, answer_vocab_size)\n",
        "\n",
        "        print(f\"[INFO] Initialized VQAModel with {resnet_type}, hidden size {hidden_size}\")\n",
        "\n",
        "    def _get_resnet_model(self, resnet_type, hidden_size):\n",
        "        # Dynamically fetch the ResNet model\n",
        "        resnet_constructor = getattr(models, resnet_type)\n",
        "        resnet = resnet_constructor(pretrained=True)\n",
        "\n",
        "        # Replace the last fully connected layer to output hidden_size\n",
        "        resnet.fc = nn.Linear(resnet.fc.in_features, hidden_size)\n",
        "        return resnet\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        print(\"Forward pass started.\")\n",
        "        # Get image features\n",
        "        image_features = self.resnet(images)\n",
        "\n",
        "        # Process questions\n",
        "        question_embedding = self.embedding(questions)\n",
        "        print(f\"Question embedding shape: {question_embedding.shape}\")\n",
        "\n",
        "        _, (question_features, _) = self.lstm(question_embedding)\n",
        "        print(f\"Question features from LSTM shape: {question_features.shape}\")\n",
        "\n",
        "        # Concatenate image and question features\n",
        "        combined_features = torch.cat((image_features, question_features[-1]), dim=1)\n",
        "        print(f\"Combined features shape: {combined_features.shape}\")\n",
        "\n",
        "        # Get the final output (answer prediction)\n",
        "        output = self.fc(combined_features)\n",
        "        print(f\"Model output shape: {output.shape}\")\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "g2tajyRhMu1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = VQAModel(\n",
        "#     question_vocab_size=5000,\n",
        "#     hidden_size=512,\n",
        "#     answer_vocab_size=1000,\n",
        "#     resnet_type='resnet101'  # You can change this to resnet18, resnet34, etc.\n",
        "# )\n"
      ],
      "metadata": {
        "id": "LE3z1kwCMw_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, question_vocab_size, hidden_size, answer_vocab_size,resnet_type):\n",
        "        super(VQAModel, self).__init__()\n",
        "\n",
        "        # Use ResNet50 for image feature extraction\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hidden_size)\n",
        "\n",
        "        # Embedding for questions\n",
        "        self.embedding = nn.Embedding(question_vocab_size, hidden_size)\n",
        "\n",
        "        # LSTM for question processing\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "        # Final fully connected layer to predict the answer\n",
        "        self.fc = nn.Linear(hidden_size * 2, answer_vocab_size)\n",
        "\n",
        "        # DEBUG: Print embedding layer dimensions\n",
        "        print(f\"Embedding layer initialized with vocab size {question_vocab_size} and hidden size {hidden_size}\")\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        print(\"Forward pass started.\")\n",
        "        # Get image features\n",
        "        image_features = self.resnet(images)\n",
        "\n",
        "        # Process questions\n",
        "        question_embedding = self.embedding(questions)\n",
        "        print(f\"Question embedding shape: {question_embedding.shape}\")  # DEBUG\n",
        "\n",
        "        _, (question_features, _) = self.lstm(question_embedding)\n",
        "        print(f\"Question features from LSTM shape: {question_features.shape}\")  # DEBUG\n",
        "\n",
        "        # Concatenate image and question features\n",
        "        combined_features = torch.cat((image_features, question_features[-1]), dim=1)\n",
        "        print(f\"Combined features shape: {combined_features.shape}\")  # DEBUG\n",
        "\n",
        "        # Get the final output (answer prediction)\n",
        "        output = self.fc(combined_features)\n",
        "        print(f\"Model output shape: {output.shape}\")  # DEBUG\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "qH4xxvjINPXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "hidden_size = 256\n",
        "answer_vocab_size = 10  # As derived earlier\n",
        "model = VQAModel(question_vocab_size=37, hidden_size=hidden_size, answer_vocab_size=answer_vocab_size,resnet_type='resnet152').to(device)\n",
        "\n",
        "# model = VQAModel(\n",
        "#     question_vocab_size=5000,\n",
        "#     hidden_size=512,\n",
        "#     answer_vocab_size=1000,\n",
        "#     resnet_type='resnet101'  # You can change this to resnet18, resnet34, etc.\n",
        "# )\n",
        "\n",
        "# Define optimizer and loss function (AdamW)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# Use weights assigned to each class\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "Rjid7scYN2Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, questions, answers in train_loader:\n",
        "    images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n"
      ],
      "metadata": {
        "id": "tVrLX6caO2wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "\n",
        "def wup_similarity(pred, gt, threshold=0.8):\n",
        "    #Compute Wu-Palmer Similarity between predicted and ground truth answers\n",
        "    pred_synsets = wordnet.synsets(pred)\n",
        "    gt_synsets = wordnet.synsets(gt)\n",
        "\n",
        "    if not pred_synsets or not gt_synsets:\n",
        "        return 0  # If no synsets are found, similarity is 0\n",
        "\n",
        "    max_sim = max(wordnet.wup_similarity(p, g) or 0 for p in pred_synsets for g in gt_synsets)\n",
        "    return 1 if max_sim >= threshold else max_sim"
      ],
      "metadata": {
        "id": "1FLIxoIu86Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, num_epochs, device, save_path):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # 🔹 Training Loop\n",
        "        for images, questions, answers in train_loader:\n",
        "            images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, questions)\n",
        "            loss = criterion(outputs, answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # 🔹 Evaluation on Test Data\n",
        "        model.eval()\n",
        "        total_test_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, questions, answers in test_loader:\n",
        "                images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
        "\n",
        "                outputs = model(images, questions)\n",
        "                loss = criterion(outputs, answers)\n",
        "\n",
        "                total_test_loss += loss.item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        test_losses.append(avg_test_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "    # 🔹 Plot Training vs. Testing Loss Curve\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(range(1, num_epochs+1), train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(range(1, num_epochs+1), test_losses, label=\"Testing Loss\", marker='s')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs. Testing Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# 🔹 Run Training & Evaluation\n",
        "num_epochs = 10\n",
        "save_path = \"vqa_LSTM.pth\"\n",
        "train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, num_epochs, device, save_path)\n"
      ],
      "metadata": {
        "id": "TKaapaf6sig9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, dataloader, device, threshold=0.8):\n",
        "    model.eval()\n",
        "    all_results = []\n",
        "    all_preds, all_labels, all_wups = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:  #  Fix: Only one variable unpacking\n",
        "            if len(batch) == 3:  # If dataset returns 3 elements\n",
        "                images, questions, answers = batch\n",
        "            elif len(batch) == 4:  # If dataset returns 4 elements\n",
        "                images, questions, attention_masks, answers = batch\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected batch size: {len(batch)} elements\")\n",
        "\n",
        "            #  Move tensors to GPU/CPU\n",
        "            images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
        "\n",
        "            # 🔹 Get model outputs & probabilities\n",
        "            outputs = model(images, questions)\n",
        "            probabilities = softmax(outputs, dim=1).cpu().numpy()\n",
        "            preds = np.argmax(probabilities, axis=1)\n",
        "\n",
        "            labels = answers.cpu().numpy()\n",
        "\n",
        "            # 🔹 Compute WUPS for each prediction\n",
        "            batch_wups = [wup_similarity(str(p), str(l), threshold) for p, l in zip(preds, labels)]\n",
        "            all_wups.extend(batch_wups)\n",
        "\n",
        "            # 🔹 Store results per image\n",
        "            for i in range(len(images)):\n",
        "                result = {\n",
        "                    \"image_index\": len(all_results),\n",
        "                    \"true_label\": labels[i],\n",
        "                    \"predicted_label\": preds[i],\n",
        "                    \"class_probabilities\": probabilities[i].tolist(),\n",
        "                    \"wups_score\": batch_wups[i]  #  WUPS added\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                all_preds.append(preds[i])\n",
        "                all_labels.append(labels[i])\n",
        "\n",
        "    #  Compute Metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    wups_score = np.mean(all_wups)\n",
        "\n",
        "    #  Print Metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"WUPS Score (Threshold {threshold}): {wups_score:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"wups\": wups_score,\n",
        "        \"results_per_image\": all_results,\n",
        "    }\n",
        "\n",
        "# 🔹 Run Evaluation\n",
        "metrics = evaluate_model(model, test_loader, device)\n"
      ],
      "metadata": {
        "id": "6VV94iieDUGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(metrics[\"results_per_image\"])\n",
        "print(df_results.to_string(index=False))"
      ],
      "metadata": {
        "id": "6ghR5jWBFqvs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
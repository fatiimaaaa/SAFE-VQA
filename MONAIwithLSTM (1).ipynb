{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "id": "Y_KI5_duzk2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install monai"
      ],
      "metadata": {
        "id": "o7kDi-Q8p3_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import ast\n",
        "import monai\n",
        "from monai.transforms import Compose, LoadImage, EnsureChannelFirst, Resize, ScaleIntensity, ToTensor\n",
        "from monai.networks.nets import DenseNet121\n",
        "from monai.data import Dataset"
      ],
      "metadata": {
        "id": "Niz4r76vofdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh0IrXCT_mdV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check current working directory\n",
        "os.chdir('/content/drive/My Drive/Handxray_Dataset/handxray')\n",
        "print(\"Current Working Directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "981pHZHuq6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DataFrame\n",
        "file_path = 'VQA_QAEncoded.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "cXUmPAi1rREw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure question_encoded and answer_encoded are properly formatted\n",
        "#df['question_encoded'] = df['question_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "df['answer_encoded'] = df['answer_encoded'].astype(int)"
      ],
      "metadata": {
        "id": "k-EjwrT9G9KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure all answers in your dataset are integers and lie in the range [0, answer_vocab_size - 1]\n",
        "print(\"Unique answer labels:\", df['answer_encoded'].unique())\n",
        "print(\"Answer vocabulary size:\", len(df['answer_encoded'].unique()))"
      ],
      "metadata": {
        "id": "72gjburJHCo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure token indices start from 0\n",
        "unique_tokens = sorted(set(token for q in df['question_encoded'] for token in q))\n",
        "question_vocab = {token: idx for idx, token in enumerate(unique_tokens)}  # 0-based index\n"
      ],
      "metadata": {
        "id": "CR1Tm7SRH5Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['question_encoded'] = df['question_encoded'].apply(lambda q: [question_vocab[token] for token in q])\n"
      ],
      "metadata": {
        "id": "9OfWExXoH-4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ensure correct token mapping\n",
        "# unique_tokens = set(token for q in df['question_encoded'] for token in q)\n",
        "# question_vocab = {token: idx for idx, token in enumerate(sorted(unique_tokens))}\n"
      ],
      "metadata": {
        "id": "ej6liiWDZpPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build question_vocab from unique tokens in questions\n",
        "#question_vocab = set(token for q in df['question_encoded'] for token in q)\n",
        "#question_vocab = {token: idx for idx, token in enumerate(question_vocab)}"
      ],
      "metadata": {
        "id": "AUQwIxH4QCNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ MONAI Image Transforms (NOW DEFINED BEFORE USE)\n",
        "monai_transforms = Compose([\n",
        "    LoadImage(image_only=True),\n",
        "    EnsureChannelFirst(),\n",
        "    Resize((224, 224)),  # Adjust size based on model input requirements\n",
        "    ScaleIntensity(),  # Normalize pixel values\n",
        "    ToTensor()  # Convert to PyTorch tensor\n",
        "])"
      ],
      "metadata": {
        "id": "hK8PpvH4ONe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "        self.image_loader = LoadImage(image_only=True)\n",
        "        self.monai_transforms = Compose([\n",
        "            EnsureChannelFirst(),\n",
        "            ScaleIntensity(),\n",
        "            Resize((224, 224))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Image using MONAI\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        image = self.image_loader(img_path)\n",
        "        image = self.monai_transforms(image)\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "\n",
        "        # Load Question and Answer\n",
        "        question_encoded = self.dataframe.iloc[idx]['question_encoded']\n",
        "        attention_mask = [1] * len(question_encoded)\n",
        "\n",
        "        question = torch.tensor(question_encoded, dtype=torch.long)\n",
        "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "        answer = torch.tensor(self.dataframe.iloc[idx]['answer_encoded'], dtype=torch.long)\n",
        "\n",
        "        return image, question, attention_mask, answer"
      ],
      "metadata": {
        "id": "ZP71MjkfLRkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    images, input_ids, attention_masks, answers = zip(*batch)\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "    answers = torch.tensor(answers, dtype=torch.long)\n",
        "\n",
        "    return images, input_ids, attention_masks, answers"
      ],
      "metadata": {
        "id": "YZvrQVN4Lagr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = VQADataset(df, transform=monai_transforms)"
      ],
      "metadata": {
        "id": "zKUaGws1-6sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "6RP4H9dILsnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYIke8RMb9Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DmGq4UbPL6zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Create DataLoader\n",
        "# dataset = VQADataset(df, transform=monai_transforms)\n",
        "# data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "#Check Data Shapes\n",
        "for images, input_ids, attention_masks, answers in train_loader:\n",
        "    print(f\"Images shape: {images.shape}\")\n",
        "    print(f\"Input IDs shape: {input_ids.shape}\")\n",
        "    print(f\"Attention masks shape: {attention_masks.shape}\")\n",
        "    print(f\"Answers shape: {answers.shape}\")\n",
        "    break  # Stop after the first batch"
      ],
      "metadata": {
        "id": "N6shXLM6Nsmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MONAI_LSTM_Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, image_feature_dim=256, num_classes=10):\n",
        "        super(MONAI_LSTM_Model, self).__init__()\n",
        "\n",
        "        self.image_feature_dim = image_feature_dim  # âœ… Store image feature dim\n",
        "\n",
        "        # ðŸ”¹ Image Feature Extractor (CNN)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((8, 8))  # âœ… Output: [batch_size, 128, 8, 8]\n",
        "        )\n",
        "\n",
        "        # ðŸ”¹ Fully Connected Layer for Image Features\n",
        "        self.image_fc = nn.Linear(128 * 8 * 8, image_feature_dim)  # âœ… Match LSTM hidden_dim\n",
        "\n",
        "        # ðŸ”¹ Question Processing (LSTM)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # ðŸ”¹ Fully Connected Layers for Final Prediction\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + image_feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, questions, attention_masks):\n",
        "        # ðŸŸ¢ Process Image Features\n",
        "        image_features = self.cnn(images)\n",
        "        batch_size = images.size(0)\n",
        "        image_features = image_features.view(batch_size, -1)\n",
        "        image_features = self.image_fc(image_features)\n",
        "\n",
        "        # ðŸŸ¢ Process Question with LSTM\n",
        "        embedded = self.embedding(questions)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        hidden = hidden.squeeze(0)\n",
        "\n",
        "        # ðŸŸ¢ Concatenate Features\n",
        "        combined = torch.cat((hidden, image_features), dim=1)\n",
        "\n",
        "        # ðŸŸ¢ Final Prediction\n",
        "        output = self.fc(combined)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "wb3aziS3wLa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "9FYAyFJFyBWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Initialization\n",
        "model = MONAI_LSTM_Model(\n",
        "    vocab_size=14,  # Adjust as needed\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    image_feature_dim=256,  # Ensure it matches the processed image features\n",
        "    num_classes=10  # Adjust based on your dataset\n",
        ")\n",
        "\n",
        "# Move Model to Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Start Training\n",
        "#train(model, data_loader, criterion, optimizer, num_epochs=2)\n"
      ],
      "metadata": {
        "id": "XJiHGsUr3VTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "nhle2poi_Qj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ðŸ”¹ Define Model, Loss, and Optimizervocab_size=len(question_vocab)\n",
        "# #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = MONAI_LSTM_Model(vocab_size=len(question_vocab), num_classes=df['answer_encoded'].nunique()).to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "DkTZCX2vZLok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ðŸ”¹ Training Loop\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, input_ids, attention_masks, answers in dataloader:\n",
        "            images, input_ids, attention_masks, answers = images.to(device), input_ids.to(device), attention_masks.to(device), answers.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, input_ids, attention_masks)\n",
        "            loss = criterion(outputs, answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == answers).sum().item()\n",
        "            total += answers.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Predicted:[{predicted}]\")"
      ],
      "metadata": {
        "id": "KlfCpwejZHqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ðŸ”¹ Train Function with Loss Tracking\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    train_losses, test_losses = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        total_train_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        # ðŸ”¹ Training Loop\n",
        "        for images, input_ids, attention_masks, answers in train_loader:\n",
        "            images, input_ids, attention_masks, answers = images.to(device), input_ids.to(device), attention_masks.to(device), answers.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, input_ids, attention_masks)\n",
        "            loss = criterion(outputs, answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == answers).sum().item()\n",
        "            total_train += answers.size(0)\n",
        "\n",
        "        train_accuracy = correct_train / total_train\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # ðŸ”¹ Validation Loop (Testing)\n",
        "        model.eval()\n",
        "        total_test_loss = 0\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, input_ids, attention_masks, answers in test_loader:\n",
        "                images, input_ids, attention_masks, answers = images.to(device), input_ids.to(device), attention_masks.to(device), answers.to(device)\n",
        "\n",
        "                outputs = model(images, input_ids, attention_masks)\n",
        "                loss = criterion(outputs, answers)\n",
        "                total_test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_test += (predicted == answers).sum().item()\n",
        "                total_test += answers.size(0)\n",
        "\n",
        "        test_accuracy = correct_test / total_test\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        test_losses.append(avg_test_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "\n",
        "    # ðŸ”¹ Save Model\n",
        "    torch.save(model.state_dict(), \"vqa_model.pth\")\n",
        "    print(\"Model saved as vqa_model.pth\")\n",
        "\n",
        "    # ðŸ”¹ Plot Training & Validation Loss\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\", marker=\"o\")\n",
        "    plt.plot(range(1, num_epochs + 1), test_losses, label=\"Test Loss\", marker=\"s\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(\"loss_curve.png\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Qx8BbVbd2zwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(model, train_loader, test_loader, criterion, optimizer, num_epochs, device)\n"
      ],
      "metadata": {
        "id": "vPm1_dub25cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Run Training\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=10)"
      ],
      "metadata": {
        "id": "zBQg802lZPIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "\n",
        "def wup_similarity(pred, gt, threshold=0.8):\n",
        "    #Compute Wu-Palmer Similarity between predicted and ground truth answers\n",
        "    pred_synsets = wordnet.synsets(pred)\n",
        "    gt_synsets = wordnet.synsets(gt)\n",
        "\n",
        "    if not pred_synsets or not gt_synsets:\n",
        "        return 0  # If no synsets are found, similarity is 0\n",
        "\n",
        "    max_sim = max(wordnet.wup_similarity(p, g) or 0 for p in pred_synsets for g in gt_synsets)\n",
        "    return 1 if max_sim >= threshold else max_sim\n"
      ],
      "metadata": {
        "id": "uUfY4CPM1cXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, test_loader, device, threshold=0.8):\n",
        "    model.eval()\n",
        "    all_results = []  # Store per-image results\n",
        "    all_preds, all_labels, all_wups = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, input_ids, attention_masks, answers) in enumerate(test_loader):\n",
        "            images, input_ids, attention_masks, answers = images.to(device), input_ids.to(device), attention_masks.to(device), answers.to(device)\n",
        "\n",
        "            # ðŸ”¹ Get model outputs and apply Softmax for probabilities\n",
        "            outputs = model(images, input_ids, attention_masks)\n",
        "            probabilities = softmax(outputs, dim=1).cpu().numpy()  # Convert to NumPy array\n",
        "            preds = np.argmax(probabilities, axis=1)\n",
        "\n",
        "            labels = answers.cpu().numpy()\n",
        "\n",
        "            # Compute WUPS for each prediction\n",
        "            batch_wups = [wup_similarity(str(p), str(l), threshold) for p, l in zip(preds, labels)]\n",
        "            all_wups.extend(batch_wups)\n",
        "\n",
        "            # ðŸ”¹ Store results per image\n",
        "            for i in range(len(images)):\n",
        "                result = {\n",
        "                    \"image_index\": batch_idx * len(images) + i,  # Unique index for each image\n",
        "                    \"true_label\": labels[i],\n",
        "                    \"predicted_label\": preds[i],\n",
        "                    \"class_probabilities\": probabilities[i].tolist(),  # Convert to list for readability\n",
        "                    \"wups_score\": batch_wups[i]\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                all_preds.append(preds[i])\n",
        "                all_labels.append(labels[i])\n",
        "\n",
        "    # Compute Overall Metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
        "    wups_score = np.mean(all_wups)\n",
        "\n",
        "    # Print Overall Metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"WUPS Score (Threshold {threshold}): {wups_score:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"wups\": wups_score,\n",
        "        \"results_per_image\": all_results  # âœ… Detailed per-image results\n",
        "    }\n",
        "\n",
        "# ðŸ”¹ Run Evaluation\n",
        "metrics = evaluate_model(model, test_loader, device)\n"
      ],
      "metadata": {
        "id": "WZMXDToGHnEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(metrics[\"results_per_image\"])\n",
        "print(df_results.to_string(index=False))"
      ],
      "metadata": {
        "id": "TiUeLddaUnFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Convert to table format\n",
        "table = tabulate(metrics[\"results_per_image\"], headers=\"keys\", tablefmt=\"grid\")\n",
        "\n",
        "# Print the table\n",
        "print(table)\n"
      ],
      "metadata": {
        "id": "2iUSQ-I0WDBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Run Evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "OYw8I9vm73mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging: Check max index in `question_encoded`\n",
        "max_token_idx = max(max(q) for q in df['question_encoded'])  # Find highest token index\n",
        "print(\"Max token index:\", max_token_idx)\n",
        "print(\"Vocab size:\", len(question_vocab))\n",
        "\n",
        "# Ensure all tokens are within range\n",
        "assert max_token_idx < len(question_vocab), \"ERROR: Some question tokens exceed vocab size!\"\n"
      ],
      "metadata": {
        "id": "VQXojZ3aVLag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}